{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some considerations\n",
    "\n",
    "- Neural Inpainting : on actual license plates, its probable that due to noise/perspective etc. , when we extract single characters, parts of the characters will be missing. We could try to have a neural inpainting type of model, where we hide parts of the characters in the synthetic dataset and train the model to reconstruct these\n",
    "\n",
    "- from : https://www.linkedin.com/advice/0/how-can-you-use-variational-autoencoders-xxipf#:~:text=Variational%20autoencoders%20(VAEs)%20can%20restore,latent%20space%2C%20capturing%20essential%20features.\n",
    "\n",
    "I have utilized VAEs in my projects, and the way they reconstruct the image is incredible. AEs for image restoration involve training two encoders: one for real and synthetically degraded images, and another for clean images. The encoders compress images into a shared latent space and a separate clean space, respectively. The network then learns a mapping between these spaces using synthetic image pairs, allowing it to \"translate\" the latent representation of a degraded image towards the clean space. Finally, a decoder utilizes this translated representation to reconstruct a restored image, aiming to be closer to the original.\n",
    "\n",
    "vAE image reconstruction : \n",
    "\n",
    "https://arxiv.org/pdf/2305.02541\n",
    "https://github.com/mdhabibi/DeepLearning-VAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this code, I will consider a variational AE that should reconstruct the actual characters based on corrupted/noisy/partial characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST TRIAL :\n",
    "\n",
    "- only one vAE, that takes both clean & noisy images as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(LicensePlateVAE, self).__init__()\n",
    "        \n",
    "        # Encoder for both clean and noisy images\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input: 1x28x28\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # 64 * 7 * 7\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Latent space ; both have their own respective linear layer \n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_var = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 7, 7)), # unflatten into 64x7x7 tensor\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 28x28\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Output should be between 0 and 1 for binary images (i.e the pixel values)\n",
    "            # TODO : check if actually our input 28x28 are between 0,1 or -1,1, else sigmoid should become tanh\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        # Create std and eps of a Gaussian distribution\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        # randn_like creates a tensor with the same shape as std (or mu, they have the same shape), with values sampled from a standard normal distribution\n",
    "        eps = torch.randn_like(std) \n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function_multi_target(recon_x, clean_targets, mu, log_var, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss function that handles multiple clean targets for each noisy input\n",
    "    \n",
    "    Args:\n",
    "        recon_x: Reconstructed image (B x 1 x 28 x 28)\n",
    "        clean_targets: Multiple clean targets for each input (B x N x 1 x 28 x 28)\n",
    "        where N is the number of clean targets per noisy input\n",
    "        mu, log_var: Latent distributions\n",
    "    \"\"\"\n",
    "    # Calculate BCE for each clean target\n",
    "    bce_losses = []\n",
    "    for i in range(clean_targets.size(1)):  # Iterate over N targets\n",
    "        target = clean_targets[:, i]\n",
    "        # Binary cross entropy useful here, as we have binary (black/white) images and\n",
    "        # In the case of images, each pixel value is treated as a probability between 0 (black) and 1 (white)\n",
    "        # thus, we measure the likelihood of the reconstructed image given the target image\n",
    "        bce = F.binary_cross_entropy(recon_x, target, reduction='sum')\n",
    "        bce_losses.append(bce)\n",
    "    \n",
    "    # Take minimum BCE across all targets (meaning we choose the target that is closest to the reconstructed image, i.e. the best fit)\n",
    "    # We don't care for the vAE to generate a certain clean picture : We just want it to generate a clean picture, such that we can\n",
    "    # see which character it is !\n",
    "    BCE = torch.min(torch.stack(bce_losses))\n",
    "    \n",
    "    # KL divergence remains the same\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Possible optimization for KLD :\n",
    "    #https://discuss.pytorch.org/t/correct-implementation-of-vae-loss/146750\n",
    "    # KLD = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(),dim=1),dim=0)\n",
    "    \n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_multi_target(model, train_loader, optimizer, device, epoch, beta=1.0):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (noisy, clean_targets) in enumerate(train_loader):\n",
    "        # noisy: [B x 1 x 28 x 28]\n",
    "        # clean_targets: [B x N x 1 x 28 x 28] where N is num_clean_targets\n",
    "        noisy = noisy.to(device)\n",
    "        clean_targets = clean_targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with noisy input\n",
    "        recon_batch, mu, log_var = model(noisy)\n",
    "        \n",
    "        # Calculate loss using multiple clean targets\n",
    "        loss = vae_loss_function_multi_target(recon_batch, clean_targets, mu, log_var, beta)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(noisy)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(noisy):.6f}')\n",
    "    \n",
    "    return train_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_multi_target(model, train_loader, val_loader, optimizer, device, num_epochs, beta=1.0, patience=5):\n",
    "    \"\"\"\n",
    "    Train VAE with validation and early stopping\n",
    "    \n",
    "    Args:\n",
    "        model: VAE model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        optimizer: Optimizer\n",
    "        device: Device to run on\n",
    "        num_epochs: Maximum number of epochs\n",
    "        beta: Weight of KL divergence term\n",
    "        patience: Number of epochs to wait for improvement before early stopping\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (noisy, clean_targets) in enumerate(train_loader):\n",
    "            noisy = noisy.to(device)\n",
    "            clean_targets = clean_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, log_var = model(noisy)\n",
    "            loss = vae_loss_function_multi_target(recon_batch, clean_targets, mu, log_var, beta)\n",
    "            \n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(noisy)}/{len(train_loader.dataset)} '\n",
    "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(noisy):.6f}')\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for noisy, clean_targets in val_loader:\n",
    "                noisy = noisy.to(device)\n",
    "                clean_targets = clean_targets.to(device)\n",
    "                \n",
    "                recon_batch, mu, log_var = model(noisy)\n",
    "                loss = vae_loss_function_multi_target(recon_batch, clean_targets, mu, log_var, beta)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'====> Epoch: {epoch} Average train loss: {avg_train_loss:.4f}')\n",
    "        print(f'====> Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'beta': beta\n",
    "            }, '/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/vAE_model/license_plate_vae.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize training progress\n",
    "def plot_training_progress(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plot training and validation losses\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## NOT TESTED !\n",
    "\n",
    "# Function to visualize reconstructions\n",
    "def visualize_reconstructions(model, val_loader, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize some example reconstructions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean_targets in val_loader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean_targets = clean_targets.to(device)\n",
    "            \n",
    "            recon_batch, _, _ = model(noisy)\n",
    "            \n",
    "            # Plot results\n",
    "            fig, axes = plt.subplots(3, num_examples, figsize=(15, 6))\n",
    "            for i in range(num_examples):\n",
    "                # Plot noisy input\n",
    "                axes[0, i].imshow(noisy[i].cpu().squeeze(), cmap='gray')\n",
    "                axes[0, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[0, i].set_title('Noisy Input')\n",
    "                \n",
    "                # Plot reconstruction\n",
    "                axes[1, i].imshow(recon_batch[i].cpu().squeeze(), cmap='gray')\n",
    "                axes[1, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[1, i].set_title('Reconstruction')\n",
    "                \n",
    "                # Plot one clean target\n",
    "                axes[2, i].imshow(clean_targets[i, 0].cpu().squeeze(), cmap='gray')\n",
    "                axes[2, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[2, i].set_title('Clean Target')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateDatasetMultiTarget(Dataset):\n",
    "    def __init__(self, noisy_dir, clean_dir, transform=None):\n",
    "        self.noisy_dir = noisy_dir\n",
    "        self.clean_base_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        self.clean_path = Path(clean_dir)\n",
    "        self.noisy_path = Path(noisy_dir)\n",
    "        \n",
    "        # Get all noisy image files\n",
    "\n",
    "        self.noisy_files = []\n",
    "        for folder in self.noisy_path.iterdir():\n",
    "            if folder.is_dir():\n",
    "                self.noisy_files.extend(list(folder.glob('*.png')))\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.noisy_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        noisy_name = self.noisy_files[idx]\n",
    "        char_class = noisy_name.name.split('_')[0]  # filenames have format like \"F_1.png\"\n",
    "        \n",
    "        # Load noisy image\n",
    "        noisy_img = Image.open(self.noisy_path / char_class / noisy_name)\n",
    "        \n",
    "        clean_images = []\n",
    "        # Find the corresponding clean images to the current noisy image\n",
    "        curr_clean_path = self.clean_path / char_class\n",
    "     \n",
    "\n",
    "        for img in list(curr_clean_path.glob('*.png')):\n",
    "            clean_img = Image.open(img)\n",
    "            if self.transform:\n",
    "                clean_img = self.transform(clean_img)\n",
    "            clean_images.append(clean_img)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_img = self.transform(noisy_img)\n",
    "            \n",
    "        # Stack clean images into a single tensor\n",
    "        clean_tensor = torch.stack(clean_images)\n",
    "        \n",
    "        return noisy_img, clean_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_folders(clean_dir, noisy_dir):\n",
    "    \"\"\"Function to create train, validation, and test folders\n",
    "       The idea is to take 80%/10%/10% of each characters folder for train,validation and test respectively\"\"\"\n",
    "    \n",
    "    # First check if folders exist and delete them\n",
    "    # TODO : maybe remove later, for now just so we don't keep different images in these folders when we try diff. image augmentation methods etc. \n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        if os.path.exists(os.path.join(clean_dir, folder)):\n",
    "            shutil.rmtree(os.path.join(clean_dir, folder))\n",
    "        if os.path.exists(os.path.join(noisy_dir, folder)):\n",
    "            shutil.rmtree(os.path.join(noisy_dir, folder))\n",
    "\n",
    "    # Create folders (if not existent yet)\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(clean_dir, folder), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, folder), exist_ok=True)\n",
    "\n",
    "\n",
    "    characters = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    # and also create folders for each of the possible characters\n",
    "    for char in characters:\n",
    "        os.makedirs(os.path.join(clean_dir, 'train', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(clean_dir, 'val', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(clean_dir, 'test', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, 'train', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, 'val', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, 'test', char), exist_ok=True)\n",
    "\n",
    "    train_clean = {}\n",
    "    val_clean = {}\n",
    "    test_clean = {}\n",
    "    train_noisy = {}\n",
    "    val_noisy = {}\n",
    "    test_noisy = {}\n",
    "    clean_path = Path(clean_dir)\n",
    "    noisy_path = Path(noisy_dir)\n",
    "    # Iterate through all clean folders (i.e. each character)\n",
    "    for folder in clean_path.iterdir():\n",
    "        if folder.is_dir(): \n",
    "            files_train = (list(folder.glob('*.png')))[:80]\n",
    "            files_val = (list(folder.glob('*.png')))[80:90]\n",
    "            files_test = (list(folder.glob('*.png')))[90:100]\n",
    "            # Store in dictionary for now \n",
    "            train_clean[folder.name] = files_train\n",
    "            val_clean[folder.name] = files_val\n",
    "            test_clean[folder.name] = files_test\n",
    "    \n",
    "    # For noisy images\n",
    "    for folder in noisy_path.iterdir():\n",
    "        if folder.is_dir(): \n",
    "            files_train = (list(folder.glob('*.png')))[:80]\n",
    "            files_val = (list(folder.glob('*.png')))[80:90]\n",
    "            files_test = (list(folder.glob('*.png')))[90:100]\n",
    "            # Store in dictionary for now\n",
    "            train_noisy[folder.name] = files_train\n",
    "            val_noisy[folder.name] = files_val\n",
    "            test_noisy[folder.name] = files_test\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        # Copy files to respective folders (clean)\n",
    "        for char, files in train_clean.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, clean_path / 'train' / char / file.name)\n",
    "        for char, files in val_clean.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, clean_path / 'val' / char / file.name)\n",
    "        for char, files in test_clean.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, clean_path / 'test' / char / file.name)\n",
    "\n",
    "\n",
    "        # Copy files to respective folders (noisy)\n",
    "        for char, files in train_noisy.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, noisy_path / 'train' / char / file.name)\n",
    "        for char, files in val_noisy.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, noisy_path / 'val' / char / file.name)\n",
    "        for char, files in test_noisy.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, noisy_path / 'test' / char / file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    latent_dim = 32\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "  \n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    dataset_train = LicensePlateDatasetMultiTarget(\n",
    "        clean_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean/train',\n",
    "        noisy_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy/train',\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    dataset_val = LicensePlateDatasetMultiTarget(\n",
    "        clean_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean/val',\n",
    "        noisy_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy/val',\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    dataset_test = LicensePlateDatasetMultiTarget(\n",
    "        clean_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean/test',\n",
    "        noisy_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy/test',\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Get some element from the Training DataLoader (for testing)\n",
    "   # train_loader_iter = iter(train_loader)\n",
    "   # noisy, clean_targets = next(train_loader_iter)\n",
    "\n",
    "   # print(noisy.shape)\n",
    "   # print(clean_targets.shape)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LicensePlateVAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses = train_vae_multi_target(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=100,\n",
    "        beta=1.0, # for KL divergence \n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # Plot training progress\n",
    "    plot_training_progress(train_losses, val_losses)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), '/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/vAE_model/license_plate_vae.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2880 (0%)]\tLoss: 527.134094\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[37], line 42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vae_multi_target\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====> Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Average loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mtrain_vae_multi_target\u001b[0;34m(model, train_loader, optimizer, device, epoch, beta)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (noisy, clean_targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# noisy: [B x 1 x 28 x 28]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# clean_targets: [B x N x 1 x 28 x 28] where N is num_clean_targets\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     noisy \u001b[38;5;241m=\u001b[39m noisy\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     clean_targets \u001b[38;5;241m=\u001b[39m clean_targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[31], line 37\u001b[0m, in \u001b[0;36mLicensePlateDatasetMultiTarget.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m     clean_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 37\u001b[0m         clean_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     clean_images\u001b[38;5;241m.\u001b[39mappend(clean_img)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/Image.py:747\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 747\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/Image.py:796\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    794\u001b[0m     encoder_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/ImageFile.py:250\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m err_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# initialize to unknown error\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap:\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# sort tiles in file order\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/PngImagePlugin.py:978\u001b[0m, in \u001b[0;36mPngImageFile.load_prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderconfig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderconfig \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m,)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__prepare_idat  \u001b[38;5;66;03m# used by load_read()\u001b[39;00m\n\u001b[0;32m--> 978\u001b[0m \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/ImageFile.py:326\u001b[0m, in \u001b[0;36mImageFile.load_prepare\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# create image memory if necessary\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mnew(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# create palette (optional)\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/Image.py:567\u001b[0m, in \u001b[0;36mImage.im\u001b[0;34m(self, im)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_im\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;129m@im\u001b[39m\u001b[38;5;241m.\u001b[39msetter\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mim\u001b[39m(\u001b[38;5;28mself\u001b[39m, im: core\u001b[38;5;241m.\u001b[39mImagingCore) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_im \u001b[38;5;241m=\u001b[39m im\n\u001b[1;32m    571\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwidth\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:383\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# IFDEF CYTHON\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# cdef str filename;\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# DEBUG = 'code_to_debug' in frame.f_code.co_filename\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# if DEBUG: print('ENTER: trace_dispatch: %s %s %s %s' % (frame.f_code.co_filename, frame.f_lineno, event, frame.f_code.co_name))\u001b[39;00m\n\u001b[1;32m    382\u001b[0m py_db, t, additional_info, cache_skips, frame_skips_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args\n\u001b[0;32m--> 383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43madditional_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_tracing\u001b[49m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m NO_FTRACE  \u001b[38;5;66;03m# we don't wan't to trace code invoked from pydevd_frame.trace_dispatch\u001b[39;00m\n\u001b[1;32m    386\u001b[0m additional_info\u001b[38;5;241m.\u001b[39mis_tracing \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_train_val_test_folders('/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean', '/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs_project_ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
