{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some considerations\n",
    "\n",
    "- Neural Inpainting : on actual license plates, its probable that due to noise/perspective etc. , when we extract single characters, parts of the characters will be missing. We could try to have a neural inpainting type of model, where we hide parts of the characters in the synthetic dataset and train the model to reconstruct these\n",
    "\n",
    "- from : https://www.linkedin.com/advice/0/how-can-you-use-variational-autoencoders-xxipf#:~:text=Variational%20autoencoders%20(VAEs)%20can%20restore,latent%20space%2C%20capturing%20essential%20features.\n",
    "\n",
    "I have utilized VAEs in my projects, and the way they reconstruct the image is incredible. AEs for image restoration involve training two encoders: one for real and synthetically degraded images, and another for clean images. The encoders compress images into a shared latent space and a separate clean space, respectively. The network then learns a mapping between these spaces using synthetic image pairs, allowing it to \"translate\" the latent representation of a degraded image towards the clean space. Finally, a decoder utilizes this translated representation to reconstruct a restored image, aiming to be closer to the original.\n",
    "\n",
    "vAE image reconstruction : \n",
    "\n",
    "https://arxiv.org/pdf/2305.02541\n",
    "https://github.com/mdhabibi/DeepLearning-VAE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this code, I will consider a variational AE that should reconstruct the actual characters based on corrupted/noisy/partial characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST TRIAL :\n",
    "\n",
    "- only one vAE, that takes both clean & noisy images as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super(LicensePlateVAE, self).__init__()\n",
    "        \n",
    "        # Encoder for both clean and noisy images\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input: 1x28x28\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),  # 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(), # 64 * 7 * 7\n",
    "\n",
    "        )\n",
    "        \n",
    "        # Latent space ; both have their own respective linear layer \n",
    "        self.fc_mu = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        self.fc_var = nn.Linear(64 * 7 * 7, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim, 64 * 7 * 7)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Unflatten(1, (64, 7, 7)), # unflatten into 64x7x7 tensor\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 28x28\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Output should be between 0 and 1 for binary images (i.e the pixel values)\n",
    "            # TODO : check if actually our input 28x28 are between 0,1 or -1,1, else sigmoid should become tanh\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mu = self.fc_mu(x)\n",
    "        log_var = self.fc_var(x)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        # Create std and eps of a Gaussian distribution\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        # randn_like creates a tensor with the same shape as std (or mu, they have the same shape), with values sampled from a standard normal distribution\n",
    "        eps = torch.randn_like(std) \n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        x = self.decoder_input(z)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z), mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function_multi_target(recon_x, clean_targets, mu, log_var, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss function that handles multiple clean targets for each noisy input\n",
    "    \n",
    "    Args:\n",
    "        recon_x: Reconstructed image (B x 1 x 28 x 28)\n",
    "        clean_targets: Multiple clean targets for each input (B x N x 1 x 28 x 28)\n",
    "        where N is the number of clean targets per noisy input\n",
    "        mu, log_var: Latent distributions\n",
    "    \"\"\"\n",
    "    # Calculate BCE for each clean target\n",
    "    bce_losses = []\n",
    "    for i in range(clean_targets.size(1)):  # Iterate over N targets\n",
    "        target = clean_targets[:, i]\n",
    "        # Binary cross entropy useful here, as we have binary (black/white) images and\n",
    "        # In the case of images, each pixel value is treated as a probability between 0 (black) and 1 (white)\n",
    "        # thus, we measure the likelihood of the reconstructed image given the target image\n",
    "        bce = F.binary_cross_entropy(recon_x, target, reduction='sum')\n",
    "        bce_losses.append(bce)\n",
    "    \n",
    "    # Take minimum BCE across all targets (meaning we choose the target that is closest to the reconstructed image, i.e. the best fit)\n",
    "    # We don't care for the vAE to generate a certain clean picture : We just want it to generate a clean picture, such that we can\n",
    "    # see which character it is !\n",
    "    # TODO : try .mean() ? \n",
    "    BCE = torch.min(torch.stack(bce_losses))\n",
    "    \n",
    "    # KL divergence remains the same\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "\n",
    "    # Possible optimization for KLD :\n",
    "    #https://discuss.pytorch.org/t/correct-implementation-of-vae-loss/146750\n",
    "    # KLD = torch.mean(-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(),dim=1),dim=0)\n",
    "    \n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vae_multi_target(model, train_loader, val_loader, optimizer, device, num_epochs, beta=1.0, patience=5):\n",
    "    \"\"\"\n",
    "    Train VAE with validation and early stopping\n",
    "    \n",
    "    Args:\n",
    "        model: VAE model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        optimizer: Optimizer\n",
    "        device: Device to run on\n",
    "        num_epochs: Maximum number of epochs\n",
    "        beta: Weight of KL divergence term\n",
    "        patience: Number of epochs to wait for improvement before early stopping\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        \n",
    "        for batch_idx, (noisy, clean_targets) in enumerate(train_loader):\n",
    "            noisy = noisy.to(device)\n",
    "            clean_targets = clean_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, log_var = model(noisy)\n",
    "            loss = vae_loss_function_multi_target(recon_batch, clean_targets, mu, log_var, beta)\n",
    "            \n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(noisy)}/{len(train_loader.dataset)} '\n",
    "                      f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item() / len(noisy):.6f}')\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for noisy, clean_targets in val_loader:\n",
    "                noisy = noisy.to(device)\n",
    "                clean_targets = clean_targets.to(device)\n",
    "                \n",
    "                recon_batch, mu, log_var = model(noisy)\n",
    "                loss = vae_loss_function_multi_target(recon_batch, clean_targets, mu, log_var, beta)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f'====> Epoch: {epoch} Average train loss: {avg_train_loss:.4f}')\n",
    "        print(f'====> Epoch: {epoch} Average validation loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': avg_train_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "                'beta': beta\n",
    "            }, '/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/vAE_model/license_plate_vae.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize training progress\n",
    "def plot_training_progress(train_losses, val_losses):\n",
    "    \"\"\"\n",
    "    Plot training and validation losses\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize reconstructions\n",
    "def visualize_reconstructions(model, val_loader, device, num_examples=5):\n",
    "    \"\"\"\n",
    "    Visualize some example reconstructions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean_targets in val_loader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean_targets = clean_targets.to(device)\n",
    "            \n",
    "            recon_batch, _, _ = model(noisy)\n",
    "            \n",
    "            # Plot results\n",
    "            fig, axes = plt.subplots(3, num_examples, figsize=(15, 6))\n",
    "            for i in range(num_examples):\n",
    "                # Plot noisy input\n",
    "                axes[0, i].imshow(noisy[i].cpu().squeeze(), cmap='gray')\n",
    "                axes[0, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[0, i].set_title('Noisy Input')\n",
    "                \n",
    "                # Plot reconstruction\n",
    "                axes[1, i].imshow(recon_batch[i].cpu().squeeze(), cmap='gray')\n",
    "                axes[1, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[1, i].set_title('Reconstruction')\n",
    "                \n",
    "                # Plot one clean target\n",
    "                axes[2, i].imshow(clean_targets[i, 0].cpu().squeeze(), cmap='gray')\n",
    "                axes[2, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[2, i].set_title('Clean Target')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicensePlateDatasetMultiTarget(Dataset):\n",
    "    def __init__(self, noisy_dir, clean_dir, transform=None):\n",
    "        self.noisy_dir = noisy_dir\n",
    "        self.clean_base_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        self.clean_path = Path(clean_dir)\n",
    "        self.noisy_path = Path(noisy_dir)\n",
    "        \n",
    "        # Get all noisy image files\n",
    "\n",
    "        self.noisy_files = []\n",
    "        for folder in self.noisy_path.iterdir():\n",
    "            if folder.is_dir():\n",
    "                self.noisy_files.extend(list(folder.glob('*.png')))\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.noisy_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        noisy_name = self.noisy_files[idx]\n",
    "        char_class = noisy_name.name.split('_')[0]  # filenames have format like \"F_1.png\"\n",
    "        \n",
    "        # Load noisy image\n",
    "        noisy_img = Image.open(self.noisy_path / char_class / noisy_name)\n",
    "        \n",
    "        clean_images = []\n",
    "        # Find the corresponding clean images to the current noisy image\n",
    "        curr_clean_path = self.clean_path / char_class\n",
    "     \n",
    "\n",
    "        for img in list(curr_clean_path.glob('*.png')):\n",
    "            clean_img = Image.open(img)\n",
    "            if self.transform:\n",
    "                clean_img = self.transform(clean_img)\n",
    "            clean_images.append(clean_img)\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_img = self.transform(noisy_img)\n",
    "            \n",
    "        # Stack clean images into a single tensor\n",
    "        clean_tensor = torch.stack(clean_images)\n",
    "        \n",
    "        return noisy_img, clean_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_test_folders(clean_dir, noisy_dir):\n",
    "    \"\"\"Function to create train, validation, and test folders\n",
    "       The idea is to take 80%/10%/10% of each characters folder for train,validation and test respectively\"\"\"\n",
    "    \n",
    "    # First check if folders exist and delete them\n",
    "    # TODO : maybe remove later, for now just so we don't keep different images in these folders when we try diff. image augmentation methods etc. \n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        if os.path.exists(os.path.join(clean_dir, folder)):\n",
    "            shutil.rmtree(os.path.join(clean_dir, folder))\n",
    "        if os.path.exists(os.path.join(noisy_dir, folder)):\n",
    "            shutil.rmtree(os.path.join(noisy_dir, folder))\n",
    "\n",
    "    # Create folders (if not existent yet)\n",
    "    for folder in ['train', 'val', 'test']:\n",
    "        os.makedirs(os.path.join(clean_dir, folder), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, folder), exist_ok=True)\n",
    "\n",
    "\n",
    "    characters = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "    # and also create folders for each of the possible characters\n",
    "    for char in characters:\n",
    "        os.makedirs(os.path.join(clean_dir, 'train', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(clean_dir, 'val', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(clean_dir, 'test', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, 'train', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, 'val', char), exist_ok=True)\n",
    "        os.makedirs(os.path.join(noisy_dir, 'test', char), exist_ok=True)\n",
    "\n",
    "    train_clean = {}\n",
    "    val_clean = {}\n",
    "    test_clean = {}\n",
    "    train_noisy = {}\n",
    "    val_noisy = {}\n",
    "    test_noisy = {}\n",
    "    clean_path = Path(clean_dir)\n",
    "    noisy_path = Path(noisy_dir)\n",
    "    # Iterate through all clean folders (i.e. each character)\n",
    "    for folder in clean_path.iterdir():\n",
    "        if folder.is_dir(): \n",
    "            files_train = (list(folder.glob('*.png')))[:80]\n",
    "            files_val = (list(folder.glob('*.png')))[80:90]\n",
    "            files_test = (list(folder.glob('*.png')))[90:100]\n",
    "            # Store in dictionary for now \n",
    "            train_clean[folder.name] = files_train\n",
    "            val_clean[folder.name] = files_val\n",
    "            test_clean[folder.name] = files_test\n",
    "    \n",
    "    # For noisy images\n",
    "    for folder in noisy_path.iterdir():\n",
    "        if folder.is_dir(): \n",
    "            files_train = (list(folder.glob('*.png')))[:80]\n",
    "            files_val = (list(folder.glob('*.png')))[80:90]\n",
    "            files_test = (list(folder.glob('*.png')))[90:100]\n",
    "            # Store in dictionary for now\n",
    "            train_noisy[folder.name] = files_train\n",
    "            val_noisy[folder.name] = files_val\n",
    "            test_noisy[folder.name] = files_test\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        # Copy files to respective folders (clean)\n",
    "        for char, files in train_clean.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, clean_path / 'train' / char / file.name)\n",
    "        for char, files in val_clean.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, clean_path / 'val' / char / file.name)\n",
    "        for char, files in test_clean.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, clean_path / 'test' / char / file.name)\n",
    "\n",
    "\n",
    "        # Copy files to respective folders (noisy)\n",
    "        for char, files in train_noisy.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, noisy_path / 'train' / char / file.name)\n",
    "        for char, files in val_noisy.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, noisy_path / 'val' / char / file.name)\n",
    "        for char, files in test_noisy.items():\n",
    "            for file in files:\n",
    "                shutil.copy2(file, noisy_path / 'test' / char / file.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for noisy, clean_targets in test_loader:\n",
    "            noisy = noisy.to(device)\n",
    "            clean_targets = clean_targets.to(device)\n",
    "            \n",
    "            recon_batch, mu, log_var = model(noisy)\n",
    "            loss = vae_loss_function_multi_target(recon_batch, clean_targets, mu, log_var)\n",
    "            test_loss += loss.item()\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    print(f'====> Test set loss: {avg_test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Hyperparameters\n",
    "    latent_dim = 32\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "  \n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    dataset_train = LicensePlateDatasetMultiTarget(\n",
    "        clean_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean/train',\n",
    "        noisy_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy/train',\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    dataset_val = LicensePlateDatasetMultiTarget(\n",
    "        clean_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean/val',\n",
    "        noisy_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy/val',\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    dataset_test = LicensePlateDatasetMultiTarget(\n",
    "        clean_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/clean/test',\n",
    "        noisy_dir='/Users/marlon/Desktop/sem/vs/vs_proj/VCS_Project/data/synthetic/german_font/noisy/test',\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # Get some element from the Training DataLoader (for testing)\n",
    "   # train_loader_iter = iter(train_loader)\n",
    "   # noisy, clean_targets = next(train_loader_iter)\n",
    "\n",
    "   # print(noisy.shape)\n",
    "   # print(clean_targets.shape)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LicensePlateVAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses = train_vae_multi_target(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        num_epochs=100,\n",
    "        beta=1.0, # for KL divergence \n",
    "        patience=5\n",
    "    )\n",
    "\n",
    "    # Plot training progress\n",
    "    plot_training_progress(train_losses, val_losses)\n",
    "    \n",
    "    # Show some reconstructed examples from the validation set\n",
    "    visualize_reconstructions(model, val_loader, device, num_examples=5)\n",
    "\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    evaluate_test_set(model, test_loader, device)\n",
    "\n",
    "    # Visualize some reconstructions from test set\n",
    "    visualize_reconstructions(model, test_loader, device, num_examples=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/2880 (0%)]\tLoss: 533.401428\n",
      "====> Epoch: 0 Average train loss: 286.5835\n",
      "====> Epoch: 0 Average validation loss: 179.4302\n",
      "Train Epoch: 1 [0/2880 (0%)]\tLoss: 175.085358\n",
      "====> Epoch: 1 Average train loss: 173.4630\n",
      "====> Epoch: 1 Average validation loss: 170.3056\n",
      "Train Epoch: 2 [0/2880 (0%)]\tLoss: 168.686218\n",
      "====> Epoch: 2 Average train loss: 168.8624\n",
      "====> Epoch: 2 Average validation loss: 169.2220\n",
      "Train Epoch: 3 [0/2880 (0%)]\tLoss: 167.335022\n",
      "====> Epoch: 3 Average train loss: 166.8859\n",
      "====> Epoch: 3 Average validation loss: 165.2758\n",
      "Train Epoch: 4 [0/2880 (0%)]\tLoss: 166.728439\n",
      "====> Epoch: 4 Average train loss: 163.9686\n",
      "====> Epoch: 4 Average validation loss: 161.1015\n",
      "Train Epoch: 5 [0/2880 (0%)]\tLoss: 166.726425\n",
      "====> Epoch: 5 Average train loss: 161.5742\n",
      "====> Epoch: 5 Average validation loss: 160.6511\n",
      "Train Epoch: 6 [0/2880 (0%)]\tLoss: 165.367813\n",
      "====> Epoch: 6 Average train loss: 159.7809\n",
      "====> Epoch: 6 Average validation loss: 157.7828\n",
      "Train Epoch: 7 [0/2880 (0%)]\tLoss: 156.531097\n",
      "====> Epoch: 7 Average train loss: 157.8738\n",
      "====> Epoch: 7 Average validation loss: 155.4188\n",
      "Train Epoch: 8 [0/2880 (0%)]\tLoss: 156.315598\n",
      "====> Epoch: 8 Average train loss: 155.8797\n",
      "====> Epoch: 8 Average validation loss: 154.4742\n",
      "Train Epoch: 9 [0/2880 (0%)]\tLoss: 155.919647\n",
      "====> Epoch: 9 Average train loss: 154.6613\n",
      "====> Epoch: 9 Average validation loss: 151.2980\n",
      "Train Epoch: 10 [0/2880 (0%)]\tLoss: 154.206894\n",
      "====> Epoch: 10 Average train loss: 152.1311\n",
      "====> Epoch: 10 Average validation loss: 150.4640\n",
      "Train Epoch: 11 [0/2880 (0%)]\tLoss: 150.427780\n",
      "====> Epoch: 11 Average train loss: 150.4379\n",
      "====> Epoch: 11 Average validation loss: 149.3416\n",
      "Train Epoch: 12 [0/2880 (0%)]\tLoss: 154.395218\n",
      "====> Epoch: 12 Average train loss: 146.0688\n",
      "====> Epoch: 12 Average validation loss: 144.2467\n",
      "Train Epoch: 13 [0/2880 (0%)]\tLoss: 137.708633\n",
      "====> Epoch: 13 Average train loss: 142.0741\n",
      "====> Epoch: 13 Average validation loss: 140.4520\n",
      "Train Epoch: 14 [0/2880 (0%)]\tLoss: 139.834351\n",
      "====> Epoch: 14 Average train loss: 138.3404\n",
      "====> Epoch: 14 Average validation loss: 140.8432\n",
      "Train Epoch: 15 [0/2880 (0%)]\tLoss: 139.487686\n",
      "====> Epoch: 15 Average train loss: 133.5608\n",
      "====> Epoch: 15 Average validation loss: 137.0200\n",
      "Train Epoch: 16 [0/2880 (0%)]\tLoss: 134.173920\n",
      "====> Epoch: 16 Average train loss: 128.8136\n",
      "====> Epoch: 16 Average validation loss: 132.8337\n",
      "Train Epoch: 17 [0/2880 (0%)]\tLoss: 125.591522\n",
      "====> Epoch: 17 Average train loss: 123.1465\n",
      "====> Epoch: 17 Average validation loss: 131.0759\n",
      "Train Epoch: 18 [0/2880 (0%)]\tLoss: 119.283829\n",
      "====> Epoch: 18 Average train loss: 117.8542\n",
      "====> Epoch: 18 Average validation loss: 131.0991\n",
      "Train Epoch: 19 [0/2880 (0%)]\tLoss: 114.680565\n",
      "====> Epoch: 19 Average train loss: 112.9840\n",
      "====> Epoch: 19 Average validation loss: 129.6033\n",
      "Train Epoch: 20 [0/2880 (0%)]\tLoss: 103.528580\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 53\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m model, train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vae_multi_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# for KL divergence \u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Plot training progress\u001b[39;00m\n\u001b[1;32m     65\u001b[0m plot_training_progress(train_losses, val_losses)\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mtrain_vae_multi_target\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, beta, patience)\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     24\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (noisy, clean_targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     27\u001b[0m     noisy \u001b[38;5;241m=\u001b[39m noisy\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m     clean_targets \u001b[38;5;241m=\u001b[39m clean_targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[9], line 36\u001b[0m, in \u001b[0;36mLicensePlateDatasetMultiTarget.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m     clean_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 36\u001b[0m         clean_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     clean_images\u001b[38;5;241m.\u001b[39mappend(clean_img)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torchvision/transforms/transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/torchvision/transforms/functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[1;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[0;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_to_nptype\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/Image.py:747\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    745\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 747\u001b[0m     new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m], new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypestr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/Image.py:796\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m    794\u001b[0m     encoder_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m--> 796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/sem/vs/vs_proj/vs_project_ve/lib/python3.9/site-packages/PIL/ImageFile.py:314\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_end()\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_exclusive_fp_after_loading:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m LOAD_TRUNCATED_IMAGES \u001b[38;5;129;01mand\u001b[39;00m err_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# still raised if decoder fails to return anything\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs_project_ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
